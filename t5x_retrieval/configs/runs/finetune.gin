# Defaults for finetuning with train.py.
#
# You must also include a binding for MODEL.
#
# Required to be set:
#
# - MIXTURE_OR_TASK_NAME
# - MIXTURE_OR_TASK_MODULE
# - TASK_FEATURE_LENGTHS
# - TRAIN_STEPS  # includes pretrain steps
# - MODEL_DIR  # automatically set when using xm_launch
# - INITIAL_CHECKPOINT_PATH
#
# `TRAIN_STEPS` should include pre-training steps, e.g., if pre-trained ckpt
# has 1M steps, TRAIN_STEPS = 1.1M will perform 0.1M fine-tuning steps.
#
# Commonly overridden options:
# - DROPOUT_RATE
# - train/DatasetConfig.batch_size
# - train_eval/DatasetConfig.batch_size
# - infer_eval/DatasetConfig.batch_size
# - PjitPartitioner.num_partitions
# - Trainer.num_microbatches
# - USE_CACHED_TASKS: Whether to look for preprocessed SeqIO data, or preprocess
#    on the fly. Most common tasks are cached, hence this is set to True by
#    default.
from __gin__ import dynamic_registration

import __main__ as train_script

from t5x import adafactor
from t5x import partitioning
from t5x import utils
from t5x_retrieval import adafactor_utils
from t5x_retrieval import partitioning as t5xr_partitioning

include 't5x/configs/runs/finetune.gin'

BATCH_SIZE = 128

train_script.train:
  infer_eval_dataset_cfg = None
  eval_steps = 20
  eval_period = 1000  # eval frequency
  random_seed = None

train/utils.DatasetConfig:
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'train'
  shuffle = True
  seed = None  # use a new seed each run/restart
  use_cached = %USE_CACHED_TASKS
  pack = False
  module = %MIXTURE_OR_TASK_MODULE

train_eval/utils.DatasetConfig:
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'validation'
  shuffle = False
  seed = 42
  use_cached = %USE_CACHED_TASKS
  pack = False
  module = %MIXTURE_OR_TASK_MODULE

utils.RestoreCheckpointConfig:
  assignment_map = ((r'state/param_states.*', None),)  # Skip optimizer states
  fallback_to_scratch = True

utils.SaveCheckpointConfig:
  period = 1000
  dtype = 'float32'
  keep = None  # keep all checkpoints

utils.create_learning_rate_scheduler:
  factors = 'linear_decay'
  base_learning_rate = 0.001
  warmup_steps = 1000

adafactor.Adafactor:
  decay_rate = 0.8
  step_offset = 0
  logical_factor_rules = @adafactor_utils.logical_factor_rules()

partitioning.PjitPartitioner:
  logical_axis_rules = @partitioning.standard_logical_axis_rules()

partitioning.standard_logical_axis_rules:
  additional_rules = @t5xr_partitioning.standard_logical_axis_rules()
